{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://gitlab.inria.fr/jls/geomstats-diffauto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation in Geomstats: Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving from Autograd to Jax: Pros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd:\n",
    "- no longer under active development,\n",
    "- tends to be too slow for medium to large-scale experiments,\n",
    "- development for running Autograd on GPUs was never completed, and therefore training is limited by the execution time of native NumPy code.\n",
    "\n",
    "Jax?\n",
    "- = successor to the Autograd library\n",
    "- = combines hardware acceleration and automatic differentiation with XLA, compiled instructions for faster linear algebra methods, often with improvements to memory usage as well. \n",
    "- function transformation jit for just-in-time compilation,\n",
    "- vmap and pmap for vectorization and parallelization, \n",
    "- run models on a GPU (or TPU) if available.\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
    "\n",
    "Jax showcases convincing computation times when training neural networks:\n",
    "<img src=\"figures/jax.png\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving from Autograd to Jax: Cons\n",
    "\n",
    "**Issue 1**: \n",
    "https://stackoverflow.com/questions/64517793/why-is-this-function-slower-in-jax-vs-numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunction_numpy_v2(C, Mi, C_new, Mi_new):\n",
    "    Wg_new = np.linalg.solve(C_new, Mi_new)\n",
    "    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n",
    "    return C_new, Mi_new, Wg_new, Cg_new\n",
    "\n",
    "@jax.jit\n",
    "def testFunction_JAX_v2(C, Mi, C_new, Mi_new):\n",
    "    Wg_new = jnp.linalg.solve(C_new, Mi_new)\n",
    "    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n",
    "    return C_new, Mi_new, Wg_new, Cg_new\n",
    "\n",
    "%timeit testFunction_numpy_v2(C, Mi, C_new, Mi_new)\n",
    "# 1000 loops, best of 3: 1.11 ms per loop\n",
    "%timeit testFunction_JAX_v2(C_jax, Mi_jax, C_new_jax, Mi_new_jax)\n",
    "# 1000 loops, best of 3: 1.35 ms per loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax can slow down numpy computations for simple functions:\n",
    "- JAX/numpy generate effectively the same short series of BLAS/LAPACK calls executed on CPUs\n",
    "- Not much room for improvement over numpy: with such small arrays JAX's overhead is apparent.\n",
    "\n",
    "**Issue 2**: \n",
    "https://gist.github.com/fehiepsi/00915b4a926f63d665a997de236fad80\n",
    "Illustration of Jax overhead:\n",
    "- Numpy has really low operation dispatch overheads \n",
    "    - years of expert engineering\n",
    "- JAX's operation dispatch overheads are high\n",
    "    - not for any design reason but because it hasn't yet been engineered to the same point.\n",
    "\n",
    "Dispatch costs:\n",
    "- are independent of the size of the arrays being operated on. \n",
    "- If dispatching a lot of small operations, JAX's dispatch overhead is large. \n",
    "- But if scaling up the data being operated on, those fixed dispatch costs aren't as big.\n",
    "\n",
    "Another way to crush dispatch overheads is to use @jit. When you call an @jit function, you only pay the dispatch cost once, no matter how many jax.numpy functions are called inside that @jit function. Plus, XLA will end-to-end optimize the whole function, including fusing operations together and optimizing memory layouts and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are computations in Geomstats favorable to Jax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /Users/ninamiolane/software/anaconda3/lib/python3.7/site-packages (0.2.12)\n",
      "Requirement already satisfied: jaxlib in /Users/ninamiolane/software/anaconda3/lib/python3.7/site-packages (0.1.65)\n",
      "Requirement already satisfied: opt-einsum in /Users/ninamiolane/.local/lib/python3.7/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: absl-py in /Users/ninamiolane/.local/lib/python3.7/site-packages (from jax) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/ninamiolane/.local/lib/python3.7/site-packages (from jax) (1.19.5)\n",
      "Requirement already satisfied: scipy in /Users/ninamiolane/software/anaconda3/lib/python3.7/site-packages (from jaxlib) (1.5.2)\n",
      "Requirement already satisfied: flatbuffers in /Users/ninamiolane/.local/lib/python3.7/site-packages (from jaxlib) (1.12)\n",
      "Requirement already satisfied: six in /Users/ninamiolane/.local/lib/python3.7/site-packages (from absl-py->jax) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot from np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "import autograd.numpy as anp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sizes = [5, 10, 50]\n",
    "sizes = small_sizes + [100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 5...\n",
      "Size 10...\n",
      "Size 50...\n",
      "Size 100...\n",
      "Size 1000...\n",
      "Size 10000...\n"
     ]
    }
   ],
   "source": [
    "timings = np.zeros((4, len(sizes)))\n",
    "                   \n",
    "for i_size, size in enumerate(sizes):\n",
    "    print(f'Size {size}...')\n",
    "    c = np.random.normal(size=(size,))\n",
    "    d = np.random.normal(size=(size,))\n",
    "    timings[0, i_size] = timeit(\n",
    "        lambda: anp.dot(c, d), number = 50000)\n",
    "    timings[1, i_size] = timeit(\n",
    "        lambda: jnp.dot(c, d), number = 50000)\n",
    "    timings[2, i_size] = timeit(\n",
    "        lambda: jax.jit(jnp.dot)(c, d), number = 50000)\n",
    "    timings[3, i_size] = timeit(\n",
    "        lambda: np.dot(c, d), number = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>size</th>\n",
       "      <th>autograd</th>\n",
       "      <th>jax</th>\n",
       "      <th>jax jit</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dot from np.arrays</td>\n",
       "      <td>5</td>\n",
       "      <td>0.096677</td>\n",
       "      <td>15.522388</td>\n",
       "      <td>13.001843</td>\n",
       "      <td>0.040756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dot from np.arrays</td>\n",
       "      <td>10</td>\n",
       "      <td>0.092427</td>\n",
       "      <td>15.271843</td>\n",
       "      <td>13.273408</td>\n",
       "      <td>0.042178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dot from np.arrays</td>\n",
       "      <td>50</td>\n",
       "      <td>0.099572</td>\n",
       "      <td>15.328912</td>\n",
       "      <td>12.941105</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dot from np.arrays</td>\n",
       "      <td>100</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>15.234657</td>\n",
       "      <td>13.249006</td>\n",
       "      <td>0.043444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dot from np.arrays</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.101783</td>\n",
       "      <td>15.883423</td>\n",
       "      <td>13.225798</td>\n",
       "      <td>0.048018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dot from np.arrays</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.181295</td>\n",
       "      <td>15.836179</td>\n",
       "      <td>13.630156</td>\n",
       "      <td>0.130876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             function   size  autograd        jax    jax jit     numpy\n",
       "0  dot from np.arrays      5  0.096677  15.522388  13.001843  0.040756\n",
       "1  dot from np.arrays     10  0.092427  15.271843  13.273408  0.042178\n",
       "2  dot from np.arrays     50  0.099572  15.328912  12.941105  0.041351\n",
       "3  dot from np.arrays    100  0.082619  15.234657  13.249006  0.043444\n",
       "4  dot from np.arrays   1000  0.101783  15.883423  13.225798  0.048018\n",
       "5  dot from np.arrays  10000  0.181295  15.836179  13.630156  0.130876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings_df = pd.DataFrame({\n",
    "    \"function\": \"dot from np.arrays\",\n",
    "    \"size\": sizes,\n",
    "    \"autograd\": timings[0, :],\n",
    "    \"jax\": timings[1, :],\n",
    "    \"jax jit\": timings[2, :],\n",
    "    \"numpy\": timings[3, :],\n",
    "})\n",
    "\n",
    "display(timings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time varies little with the size:\n",
    "\n",
    "--> Most of the computation time comes from dispatch overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot from jax arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 5...\n",
      "Size 10...\n",
      "Size 50...\n",
      "Size 100...\n",
      "Size 1000...\n",
      "Size 10000...\n"
     ]
    }
   ],
   "source": [
    "timings2 = np.zeros((4, len(sizes)))\n",
    "key0 = jax.random.PRNGKey(0)\n",
    "key1 = jax.random.PRNGKey(1)\n",
    "                   \n",
    "for i_size, size in enumerate(sizes):\n",
    "    print(f'Size {size}...')\n",
    "    c = jax.random.normal(key0, shape=(size, ))\n",
    "    d = jax.random.normal(key1, shape=(size, ))\n",
    "    timings2[0, i_size] = timeit(\n",
    "        lambda: anp.dot(c, d), number = 50000)\n",
    "    timings2[1, i_size] = timeit(\n",
    "        lambda: jnp.dot(c, d), number = 50000)\n",
    "    timings2[2, i_size] = timeit(\n",
    "        lambda: jax.jit(jnp.dot)(c, d), number = 50000)\n",
    "    timings2[3, i_size] = timeit(\n",
    "        lambda: np.dot(c, d), number = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>size</th>\n",
       "      <th>autograd</th>\n",
       "      <th>jax</th>\n",
       "      <th>jax jit</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dot from jax arrays</td>\n",
       "      <td>5</td>\n",
       "      <td>0.294678</td>\n",
       "      <td>6.010035</td>\n",
       "      <td>8.816176</td>\n",
       "      <td>0.231162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dot from jax arrays</td>\n",
       "      <td>10</td>\n",
       "      <td>0.274839</td>\n",
       "      <td>6.105139</td>\n",
       "      <td>8.838437</td>\n",
       "      <td>0.235207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dot from jax arrays</td>\n",
       "      <td>50</td>\n",
       "      <td>0.288358</td>\n",
       "      <td>6.060228</td>\n",
       "      <td>8.909856</td>\n",
       "      <td>0.243972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dot from jax arrays</td>\n",
       "      <td>100</td>\n",
       "      <td>0.293221</td>\n",
       "      <td>6.126371</td>\n",
       "      <td>8.803957</td>\n",
       "      <td>0.237043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dot from jax arrays</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>6.107768</td>\n",
       "      <td>8.760931</td>\n",
       "      <td>0.248420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dot from jax arrays</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>5.963718</td>\n",
       "      <td>8.647516</td>\n",
       "      <td>0.301663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              function   size  autograd       jax   jax jit     numpy\n",
       "0  dot from jax arrays      5  0.294678  6.010035  8.816176  0.231162\n",
       "1  dot from jax arrays     10  0.274839  6.105139  8.838437  0.235207\n",
       "2  dot from jax arrays     50  0.288358  6.060228  8.909856  0.243972\n",
       "3  dot from jax arrays    100  0.293221  6.126371  8.803957  0.237043\n",
       "4  dot from jax arrays   1000  0.298800  6.107768  8.760931  0.248420\n",
       "5  dot from jax arrays  10000  0.359755  5.963718  8.647516  0.301663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings_df2 = pd.DataFrame({\n",
    "    \"function\": \"dot from jax arrays\",\n",
    "    \"size\": sizes,\n",
    "    \"autograd\": timings2[0, :],\n",
    "    \"jax\": timings2[1, :],\n",
    "    \"jax jit\": timings2[2, :],\n",
    "    \"numpy\": timings2[3, :],\n",
    "})\n",
    "\n",
    "display(timings_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autograd and NumPy suffer from a x4-x6 slow down.\n",
    "- Jax and Jax JIT benefit from a x3 speed-up.\n",
    "- Jax and Jax Jit and still significantly slower.\n",
    "- Computational time does not depend on the size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einsum from np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 5...\n",
      "Size 10...\n",
      "Size 50...\n"
     ]
    }
   ],
   "source": [
    "timings3 = np.zeros((4, len(small_sizes)))\n",
    "                   \n",
    "for i_size, size in enumerate(small_sizes):\n",
    "    print(f'Size {size}...')\n",
    "    c = np.random.normal(size=(size, size))\n",
    "    d = np.random.normal(size=(size, size))\n",
    "    timings3[0, i_size] = timeit(\n",
    "        lambda: anp.einsum(\"ni,ni->ni\", c, d), number = 50000)\n",
    "    timings3[1, i_size] = timeit(\n",
    "        lambda: jnp.einsum(\"ni,ni->ni\", c, d), number = 50000)\n",
    "# Note: Can't pass strings to jit decorated functions\n",
    "#     timings3[2, i_size] = timeit(\n",
    "#         lambda: jax.jit(jnp.einsum)(\"ni,ni->ni\", c, d), number = 50000)\n",
    "    timings3[3, i_size] = timeit(\n",
    "        lambda: np.einsum(\"ni,ni->ni\", c, d), number = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>size</th>\n",
       "      <th>autograd</th>\n",
       "      <th>jax</th>\n",
       "      <th>jax jit</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>einsum for np arrays</td>\n",
       "      <td>5</td>\n",
       "      <td>0.178299</td>\n",
       "      <td>3.875827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>einsum for np arrays</td>\n",
       "      <td>10</td>\n",
       "      <td>0.175369</td>\n",
       "      <td>3.833410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>einsum for np arrays</td>\n",
       "      <td>50</td>\n",
       "      <td>0.266010</td>\n",
       "      <td>4.266139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               function  size  autograd       jax  jax jit     numpy\n",
       "0  einsum for np arrays     5  0.178299  3.875827      NaN  0.111117\n",
       "1  einsum for np arrays    10  0.175369  3.833410      NaN  0.112446\n",
       "2  einsum for np arrays    50  0.266010  4.266139      NaN  0.202558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings_df3 = pd.DataFrame({\n",
    "    \"function\": \"einsum for np arrays\",\n",
    "    \"size\": small_sizes,\n",
    "    \"autograd\": timings3[0, :],\n",
    "    \"jax\": timings3[1, :],\n",
    "    \"jax jit\": np.nan,\n",
    "    \"numpy\": timings3[3, :],\n",
    "})\n",
    "\n",
    "display(timings_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einsum from Jax arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 5...\n",
      "Size 10...\n",
      "Size 50...\n"
     ]
    }
   ],
   "source": [
    "timings4 = np.zeros((4, len(small_sizes)))\n",
    "key0 = jax.random.PRNGKey(0)\n",
    "key1 = jax.random.PRNGKey(1)\n",
    "                   \n",
    "for i_size, size in enumerate(small_sizes):\n",
    "    print(f'Size {size}...')\n",
    "    c = jax.random.normal(key0, shape=(size, size))\n",
    "    d = jax.random.normal(key1, shape=(size, size))\n",
    "    timings4[0, i_size] = timeit(\n",
    "        lambda: anp.einsum(\"ni,ni->ni\", c, d), number = 50000)\n",
    "    timings4[1, i_size] = timeit(\n",
    "        lambda: jnp.einsum(\"ni,ni->ni\", c, d), number = 50000)\n",
    "# Note: Can't pass strings to jit decorated functions\n",
    "#     timings4[2, i_size] = timeit(\n",
    "#         lambda: jax.jit(jnp.dot)(c, d), number = 50000)\n",
    "    timings4[3, i_size] = timeit(\n",
    "        lambda: np.einsum(\"ni,ni->ni\", c, d), number = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>size</th>\n",
       "      <th>autograd</th>\n",
       "      <th>jax</th>\n",
       "      <th>jax jit</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>einsum for jax arrays</td>\n",
       "      <td>5</td>\n",
       "      <td>0.362630</td>\n",
       "      <td>3.654455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.287643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>einsum for jax arrays</td>\n",
       "      <td>10</td>\n",
       "      <td>0.378126</td>\n",
       "      <td>3.663458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.291371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>einsum for jax arrays</td>\n",
       "      <td>50</td>\n",
       "      <td>0.420035</td>\n",
       "      <td>3.680894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.354480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                function  size  autograd       jax  jax jit     numpy\n",
       "0  einsum for jax arrays     5  0.362630  3.654455      NaN  0.287643\n",
       "1  einsum for jax arrays    10  0.378126  3.663458      NaN  0.291371\n",
       "2  einsum for jax arrays    50  0.420035  3.680894      NaN  0.354480"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings_df4 = pd.DataFrame({\n",
    "    \"function\": \"einsum for jax arrays\",\n",
    "    \"size\": small_sizes,\n",
    "    \"autograd\": timings4[0, :],\n",
    "    \"jax\": timings4[1, :],\n",
    "    \"jax jit\": np.nan,\n",
    "    \"numpy\": timings4[3, :],\n",
    "})\n",
    "\n",
    "display(timings_df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eig on np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 5...\n",
      "Size 10...\n",
      "Size 50...\n"
     ]
    }
   ],
   "source": [
    "timings5 = np.zeros((4, len(small_sizes)))\n",
    "                   \n",
    "for i_size, size in enumerate(small_sizes):\n",
    "    print(f'Size {size}...')\n",
    "    c = np.random.normal(size=(size, size))\n",
    "    timings5[0, i_size] = timeit(\n",
    "        lambda: anp.linalg.eig(c), number = 50000)\n",
    "    timings5[1, i_size] = timeit(\n",
    "        lambda: jnp.linalg.eig(c), number = 50000)\n",
    "    timings5[2, i_size] = timeit(\n",
    "        lambda: jax.jit(jnp.linalg.eig)(c), number = 50000)\n",
    "    timings5[3, i_size] = timeit(\n",
    "        lambda: np.linalg.eig(c), number = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>size</th>\n",
       "      <th>autograd</th>\n",
       "      <th>jax</th>\n",
       "      <th>jax jit</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eig for np arrays</td>\n",
       "      <td>5</td>\n",
       "      <td>1.459353</td>\n",
       "      <td>9.752146</td>\n",
       "      <td>11.233599</td>\n",
       "      <td>1.426106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eig for np arrays</td>\n",
       "      <td>10</td>\n",
       "      <td>2.049612</td>\n",
       "      <td>9.819900</td>\n",
       "      <td>11.134745</td>\n",
       "      <td>2.128796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eig for np arrays</td>\n",
       "      <td>50</td>\n",
       "      <td>42.312973</td>\n",
       "      <td>31.265696</td>\n",
       "      <td>32.883014</td>\n",
       "      <td>42.454529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            function  size   autograd        jax    jax jit      numpy\n",
       "0  eig for np arrays     5   1.459353   9.752146  11.233599   1.426106\n",
       "1  eig for np arrays    10   2.049612   9.819900  11.134745   2.128796\n",
       "2  eig for np arrays    50  42.312973  31.265696  32.883014  42.454529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings_df5 = pd.DataFrame({\n",
    "    \"function\": \"eig for np arrays\",\n",
    "    \"size\": small_sizes,\n",
    "    \"autograd\": timings5[0, :],\n",
    "    \"jax\": timings5[1, :],\n",
    "    \"jax jit\": timings5[2, :],\n",
    "    \"numpy\": timings5[3, :],\n",
    "})\n",
    "\n",
    "display(timings_df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eig on jax arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 5...\n",
      "Size 10...\n",
      "Size 50...\n"
     ]
    }
   ],
   "source": [
    "timings6 = np.zeros((4, len(small_sizes)))\n",
    "key0 = jax.random.PRNGKey(0)\n",
    "                   \n",
    "for i_size, size in enumerate(small_sizes):\n",
    "    print(f'Size {size}...')\n",
    "    c = jax.random.normal(key0, shape=(size, size))\n",
    "    timings6[0, i_size] = timeit(\n",
    "        lambda: anp.linalg.eig(c), number = 50000)\n",
    "    timings6[1, i_size] = timeit(\n",
    "        lambda: jnp.linalg.eig(c), number = 50000)\n",
    "    timings6[2, i_size] = timeit(\n",
    "        lambda: jax.jit(jnp.linalg.eig)(c), number = 50000)\n",
    "    timings6[3, i_size] = timeit(\n",
    "        lambda: np.linalg.eig(c), number = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>size</th>\n",
       "      <th>autograd</th>\n",
       "      <th>jax</th>\n",
       "      <th>jax jit</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eig for jax arrays</td>\n",
       "      <td>5</td>\n",
       "      <td>1.696682</td>\n",
       "      <td>6.355492</td>\n",
       "      <td>8.260189</td>\n",
       "      <td>1.585099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eig for jax arrays</td>\n",
       "      <td>10</td>\n",
       "      <td>2.478310</td>\n",
       "      <td>6.541062</td>\n",
       "      <td>8.399578</td>\n",
       "      <td>2.304410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eig for jax arrays</td>\n",
       "      <td>50</td>\n",
       "      <td>36.830241</td>\n",
       "      <td>29.331656</td>\n",
       "      <td>28.888744</td>\n",
       "      <td>37.386768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             function  size   autograd        jax    jax jit      numpy\n",
       "0  eig for jax arrays     5   1.696682   6.355492   8.260189   1.585099\n",
       "1  eig for jax arrays    10   2.478310   6.541062   8.399578   2.304410\n",
       "2  eig for jax arrays    50  36.830241  29.331656  28.888744  37.386768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings_df6 = pd.DataFrame({\n",
    "    \"function\": \"eig for jax arrays\",\n",
    "    \"size\": small_sizes,\n",
    "    \"autograd\": timings6[0, :],\n",
    "    \"jax\": timings6[1, :],\n",
    "    \"jax jit\": timings6[2, :],\n",
    "    \"numpy\": timings6[3, :],\n",
    "})\n",
    "\n",
    "display(timings_df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More comprehensive series of tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gitlab.inria.fr/jls/geomstats-diffauto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on matrices operations (np.dot, np.linalg.svg, np.linalg.eig) jax is 4 times quicker than numpy and autograd.numpy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JL: we may raise an issue but I guess the answer we will receive is that is is a well known problem.. There are some guidelines to follow then using jax. It is quite difficult to dig into the code to verify that all requirements are met…\n",
    "eg:  https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
    "https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
    "https://jax.readthedocs.io/en/latest/faq.html#benchmarking-jax-code\n",
    "https://github.com/google/jax/issues/427\n",
    "that I understand is that jax.numpy is a rewriting of numpy, API compliant to numpy (most of the time, not 100% garantied)…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
